{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Causal Effect towards individual Resilience to risks globally\n",
    "Code authored by: Minh Kieu, UoA, 2023 <br />\n",
    "DoWhy Library: https://microsoft.github.io/dowhy/ <br />\n",
    "Data from: https://wrp.lrfoundation.org.uk\n",
    "\n",
    "#### Version 2:\n",
    "- We do not remove variables that are already in the Perception and Experience indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data modules\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import pyreadstat\n",
    "\n",
    "#Causal Discover modules\n",
    "from pgmpy.estimators import HillClimbSearch, BicScore\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "#Causal Inference modules\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import econml\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>GlobalRegion</th>\n",
       "      <th>CountryIncomeLevel2021</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>IncomeFeelings</th>\n",
       "      <th>INCOME_5</th>\n",
       "      <th>EMP_2010</th>\n",
       "      <th>Urbanicity</th>\n",
       "      <th>...</th>\n",
       "      <th>Q22A</th>\n",
       "      <th>Q22B</th>\n",
       "      <th>Q22C</th>\n",
       "      <th>Q22D</th>\n",
       "      <th>Q22E</th>\n",
       "      <th>Q23A</th>\n",
       "      <th>Q23B</th>\n",
       "      <th>Q23C</th>\n",
       "      <th>Q23D</th>\n",
       "      <th>Q23E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  GlobalRegion  CountryIncomeLevel2021   Age  Gender  \\\n",
       "0  United States           6.0                     4.0  70.0     1.0   \n",
       "1  United States           6.0                     4.0  56.0     1.0   \n",
       "\n",
       "   Education  IncomeFeelings  INCOME_5  EMP_2010  Urbanicity  ...  Q22A  Q22B  \\\n",
       "0        1.0             2.0       2.0       6.0         2.0  ...   2.0   2.0   \n",
       "1        3.0             1.0       5.0       1.0         1.0  ...   2.0   2.0   \n",
       "\n",
       "   Q22C  Q22D  Q22E  Q23A  Q23B  Q23C  Q23D  Q23E  \n",
       "0   2.0   2.0   2.0   2.0   2.0   2.0   2.0   2.0  \n",
       "1   2.0   2.0   2.0   2.0   2.0   2.0   2.0   2.0  \n",
       "\n",
       "[2 rows x 69 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, meta = pyreadstat.read_sav(\"../Data/lrf_wrp_2021_full_data.sav\")\n",
    "#filter out countries with number of data points less than 1200\n",
    "df = df.groupby('Country').filter(lambda x: x['Country'].count() >= 1200)\n",
    "df_1=df[df['Q1'].isnull()]\n",
    "#drop all columns from 'resilience_index' to 'Q5F_2019'\n",
    "df = df.drop(df.loc[:,'resilience_index':'Q5F_2019'].columns, axis=1)\n",
    "cols_to_remove = ['WPID_RANDOM','WP5','WGT','Year','country.in.both.waves','CountryIncomeLevel2019','PROJWT_2021','PROJWT_2019','AgeGroups4']\n",
    "df= df.drop(cols_to_remove, axis=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Estimate the values of Perception, Experience and Resilience index\n",
    "\n",
    "#### For given risk (eg: crime)\n",
    "$$\\text{Perception Index (given country)} = \\text{avg of individual perception scores of a country}$$\n",
    "$$\\text{Experience Index (given country)} = \\text{avg of individual experience scores of a country}$$\n",
    "$$\\text{Risk Impact Index (individual)} = \\text{experience score} \\times \\text{perception score}$$\n",
    "\n",
    "\n",
    "#### Resilience index for all seven risks\n",
    "$$\\text{Resilience Index (individual)} = \\text{Average of all the individual response to Resilience-related questions} $$\n",
    "\n",
    "- To calculate the perception score, the weitages are  very worried = 1, somewhat worried = 0.5, not worried = 0, and DK & refused = missing \n",
    "\n",
    "- To calculate the experience score, the weitages are  Both = 1, Yes,personally experienced = 0.75, Yes,know someone who has experienced = 0.5, No = 0, and DK & refused = missing \n",
    "\n",
    "### Step 1.1: First, let's estimate the Perception, Experience and Risk Impact Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlobalRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>Q4A</th>\n",
       "      <th>Q4B</th>\n",
       "      <th>Q4C</th>\n",
       "      <th>Q4D</th>\n",
       "      <th>Q4E</th>\n",
       "      <th>Q4F</th>\n",
       "      <th>Q4G</th>\n",
       "      <th>Q5A</th>\n",
       "      <th>Q5B</th>\n",
       "      <th>Q5C</th>\n",
       "      <th>Q5D</th>\n",
       "      <th>Q5E</th>\n",
       "      <th>Q5F</th>\n",
       "      <th>Q5G</th>\n",
       "      <th>Perception_index</th>\n",
       "      <th>Experience_index</th>\n",
       "      <th>Impact_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GlobalRegion        Country  Q4A  Q4B  Q4C  Q4D  Q4E  Q4F  Q4G  Q5A  Q5B  \\\n",
       "0           6.0  United States  1.0  1.0  1.0  1.0  0.5  0.0  NaN  0.0  0.0   \n",
       "1           6.0  United States  0.5  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   Q5C  Q5D  Q5E  Q5F  Q5G  Perception_index  Experience_index  Impact_index  \n",
       "0  0.0  0.0  0.0  0.0  0.0          0.750000               0.0           0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0          0.214286               0.0           0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "perception_df=df[['GlobalRegion','Country', 'Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G','Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G']].copy()\n",
    "\n",
    "# Replace Don't know and Refuse to answer response to NaN\n",
    "cols_to_replace = ['Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G','Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G']\n",
    "replace_dict = {98: np.nan, 99: np.nan}\n",
    "perception_df[cols_to_replace] = perception_df[cols_to_replace].replace(replace_dict)\n",
    "\n",
    "#removing the raws if all the columns have missing values, 98, 99 \n",
    "perception_df.dropna(subset=['Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G','Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G'], thresh=7, inplace=True)\n",
    "\n",
    "# replace the values 2 with 0.5 and 3 with 0\n",
    "perception_df.loc[:, ['Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G']] = perception_df.loc[:, ['Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G']].replace({2: 0.5, 3: 0})\n",
    "perception_df.loc[:, ['Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G']] = perception_df.loc[:, ['Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G']].replace({3: 1, 1: 0.75, 2: 0.5, 4: 0})\n",
    "\n",
    "# define a function to compute the average of the non-ignored values in each row for perception score (worry) and experience score)\n",
    "def compute_average(row):\n",
    "    valid_values = [value for value in row if not pd.isna(value)]\n",
    "    if len(valid_values) > 0:\n",
    "        return np.mean(valid_values)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# create a new column with the computed average\n",
    "perception_df['Perception_index'] = perception_df.loc[:, ['Q4A', 'Q4B','Q4C','Q4D', 'Q4E','Q4F','Q4G']].apply(compute_average, axis=1)\n",
    "perception_df['Experience_index'] = perception_df.loc[:, ['Q5A','Q5B','Q5C','Q5D','Q5E','Q5F','Q5G']].apply(compute_average, axis=1)\n",
    "perception_df['Impact_index'] = perception_df['Perception_index'] * perception_df['Experience_index']\n",
    "perception_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the individual questions related to Perception and Experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>GlobalRegion</th>\n",
       "      <th>CountryIncomeLevel2021</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>IncomeFeelings</th>\n",
       "      <th>INCOME_5</th>\n",
       "      <th>EMP_2010</th>\n",
       "      <th>Urbanicity</th>\n",
       "      <th>...</th>\n",
       "      <th>Q22D</th>\n",
       "      <th>Q22E</th>\n",
       "      <th>Q23A</th>\n",
       "      <th>Q23B</th>\n",
       "      <th>Q23C</th>\n",
       "      <th>Q23D</th>\n",
       "      <th>Q23E</th>\n",
       "      <th>Perception_index</th>\n",
       "      <th>Experience_index</th>\n",
       "      <th>Impact_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  GlobalRegion  CountryIncomeLevel2021   Age  Gender  \\\n",
       "0  United States           6.0                     4.0  70.0     1.0   \n",
       "1  United States           6.0                     4.0  56.0     1.0   \n",
       "\n",
       "   Education  IncomeFeelings  INCOME_5  EMP_2010  Urbanicity  ...  Q22D  Q22E  \\\n",
       "0        1.0             2.0       2.0       6.0         2.0  ...   2.0   2.0   \n",
       "1        3.0             1.0       5.0       1.0         1.0  ...   2.0   2.0   \n",
       "\n",
       "   Q23A  Q23B  Q23C  Q23D  Q23E  Perception_index  Experience_index  \\\n",
       "0   2.0   2.0   2.0   2.0   2.0          0.750000               0.0   \n",
       "1   2.0   2.0   2.0   2.0   2.0          0.214286               0.0   \n",
       "\n",
       "   Impact_index  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "\n",
       "[2 rows x 72 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df= df.drop(cols_to_replace, axis=1)\n",
    "df = df.join(perception_df[['Perception_index','Experience_index','Impact_index']])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Now let' estimate the value of Resilience index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlobalRegion</th>\n",
       "      <th>Country</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q10Q11Recode</th>\n",
       "      <th>Q16C</th>\n",
       "      <th>Q13</th>\n",
       "      <th>Q16D</th>\n",
       "      <th>Q16A</th>\n",
       "      <th>Q16B</th>\n",
       "      <th>Resilience_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GlobalRegion        Country  Q20  Q21  Q10Q11Recode  Q16C  Q13  Q16D  Q16A  \\\n",
       "0           6.0  United States  1.0  1.0           0.8   1.0  1.0   0.0   0.0   \n",
       "1           6.0  United States  1.0  1.0           0.8   1.0  1.0   1.0   0.0   \n",
       "\n",
       "   Q16B  Resilience_index  \n",
       "0   1.0             0.725  \n",
       "1   1.0             0.850  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_resi=df[['GlobalRegion','Country', 'Q20', 'Q21','Q10Q11Recode','Q16C', 'Q13','Q16D','Q16A','Q16B']].copy()\n",
    "\n",
    "# replace DOn't know and Refuse to answer to NaN\n",
    "cols_to_replace = ['Q20', 'Q21','Q10Q11Recode','Q16C', 'Q13','Q16D','Q16A','Q16B']\n",
    "replace_dict = {98: np.nan, 99: np.nan}\n",
    "index_resi[cols_to_replace] = index_resi[cols_to_replace].replace(replace_dict)\n",
    "\n",
    "# replace the values 2 with 0.5 and 3 with 0\n",
    "index_resi.loc[:, ['Q13']] = index_resi.loc[:, ['Q13']].replace({2: 0.5, 3: 0})\n",
    "index_resi.loc[:, ['Q10Q11Recode']] = index_resi.loc[:, ['Q10Q11Recode']].replace({1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4, 5: 0.5, 6: 0.8, 7: 0.9, 8: 1, 9: 0.7})\n",
    "index_resi.loc[:, ['Q20','Q16D','Q16A','Q16B','Q16C']] = index_resi.loc[:, ['Q20','Q16D','Q16A','Q16B','Q16C']].replace({3: 0.5, 2: 0})\n",
    "index_resi.loc[:, ['Q21']] = index_resi.loc[:, ['Q21']].replace({2: 0})\n",
    "\n",
    "# define a function to compute the average of the non-ignored values in each row for perception score (worry) and experience score)\n",
    "def compute_average(row):\n",
    "    valid_values = [value for value in row if not pd.isna(value)]\n",
    "    if len(valid_values) > 0:\n",
    "        return np.mean(valid_values)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# create a new column with the computed average\n",
    "# index['individual_score'] = index.loc[:, []].apply(compute_average, axis=1)\n",
    "#index_resi['household_score'] = index_resi.loc[:, ['Q21','Q10Q11Recode','Q16C']].apply(compute_average, axis=1)\n",
    "#index_resi['community_score'] = index_resi.loc[:, ['Q13', 'Q16D']].apply(compute_average, axis=1)\n",
    "#index_resi['society_score'] = index_resi.loc[:, ['Q16A','Q16B']].apply(compute_average, axis=1)\n",
    "\n",
    "# Calculating individual resilience score\n",
    "index_resi['Resilience_index'] = index_resi.loc[:, cols_to_replace].apply(compute_average, axis=1)\n",
    "index_resi.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>GlobalRegion</th>\n",
       "      <th>CountryIncomeLevel2021</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>IncomeFeelings</th>\n",
       "      <th>INCOME_5</th>\n",
       "      <th>EMP_2010</th>\n",
       "      <th>Urbanicity</th>\n",
       "      <th>...</th>\n",
       "      <th>Q22E</th>\n",
       "      <th>Q23A</th>\n",
       "      <th>Q23B</th>\n",
       "      <th>Q23C</th>\n",
       "      <th>Q23D</th>\n",
       "      <th>Q23E</th>\n",
       "      <th>Perception_index</th>\n",
       "      <th>Experience_index</th>\n",
       "      <th>Impact_index</th>\n",
       "      <th>Resilience_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  GlobalRegion  CountryIncomeLevel2021   Age  Gender  \\\n",
       "0  United States           6.0                     4.0  70.0     1.0   \n",
       "1  United States           6.0                     4.0  56.0     1.0   \n",
       "\n",
       "   Education  IncomeFeelings  INCOME_5  EMP_2010  Urbanicity  ...  Q22E  Q23A  \\\n",
       "0        1.0             2.0       2.0       6.0         2.0  ...   2.0   2.0   \n",
       "1        3.0             1.0       5.0       1.0         1.0  ...   2.0   2.0   \n",
       "\n",
       "   Q23B  Q23C  Q23D  Q23E  Perception_index  Experience_index  Impact_index  \\\n",
       "0   2.0   2.0   2.0   2.0          0.750000               0.0           0.0   \n",
       "1   2.0   2.0   2.0   2.0          0.214286               0.0           0.0   \n",
       "\n",
       "   Resilience_index  \n",
       "0             0.725  \n",
       "1             0.850  \n",
       "\n",
       "[2 rows x 62 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop variables that are already in the Resilience Index\n",
    "df = df.drop(['Q10','Q11_1','Q11_2','Q20', 'Q21','Q10Q11Recode','Q16C', 'Q13','Q16D','Q16A','Q16B'],axis=1)\n",
    "\n",
    "# Merge Resilience index to the main dataframe\n",
    "df = df.join(index_resi[['Resilience_index']])\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some more data cleaning by removing variables with a lot of missing values, then fill remaining NaNs with the mean value of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifically keep the Q4G\n",
    "df['Q4G']=df['Q4G'].fillna(df['Q4G'].mean())\n",
    "# then filter the rest\n",
    "df2=df.dropna(thresh=df.shape[0]*0.3,how='all',axis=1)\n",
    "df2=df2.fillna(df2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the data by converting to majorly continuous or binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['isMale'] = df2['Gender'].apply(lambda x: 1 if x == 1 else 0).astype('uint8')\n",
    "\n",
    "#Remove the don't know or refuse to answer samples\n",
    "df2['ChildrenInHousehold']=df2['ChildrenInHousehold'].astype('uint8')\n",
    "df2['Country']=df2['Country'].astype('category')\n",
    "df2['GlobalRegion']=df2['GlobalRegion'].astype('uint8')\n",
    "df2['INCOME_5']=df2['INCOME_5'].astype('uint8')\n",
    "df2['EMP_2010']=df2['EMP_2010'].astype('uint8')\n",
    "df2 = df2.loc[df2['Age']<99,:]\n",
    "df2['Age']=df2['Age'].astype('uint8')\n",
    "df2 = df2.loc[df2['Education']<9,:]\n",
    "df2['Education']=df2['Education'].astype('uint8')\n",
    "df2 = df2.loc[df2['IncomeFeelings']<5,:]\n",
    "df2['IncomeFeelings']=df2['IncomeFeelings'].astype('uint8')\n",
    "df2 = df2.loc[df2['Urbanicity']<9,:]\n",
    "df2['Urbanicity']=df2['Urbanicity'].astype('uint8')\n",
    "df2 = df2.loc[df2['Q8']<9,:]\n",
    "df2 = df2.loc[df2['Q14A']<9,:]\n",
    "df2 = df2.loc[df2['Q14B']<9,:]\n",
    "df2 = df2.loc[df2['Q14C']<9,:]\n",
    "df2 = df2.loc[df2['Q14D']<9,:]\n",
    "df2 = df2.loc[df2['Q14E']<9,:]\n",
    "df2 = df2.loc[df2['Q14F']<9,:]\n",
    "df2 = df2.loc[df2['Q14G']<9,:]\n",
    "df2 = df2.loc[df2['Q17']<9,:]\n",
    "df2 = df2.loc[df2['Q22A']<9,:]\n",
    "df2 = df2.loc[df2['Q22B']<9,:]\n",
    "df2 = df2.loc[df2['Q22C']<9,:]\n",
    "df2 = df2.loc[df2['Q22D']<9,:]\n",
    "df2 = df2.loc[df2['Q23A']<9,:]\n",
    "df2 = df2.loc[df2['Q23B']<9,:]\n",
    "df2 = df2.loc[df2['Q23C']<9,:]\n",
    "df2 = df2.loc[df2['Q23D']<9,:]\n",
    "df2 = df2.loc[df2['Q23E']<9,:]\n",
    "\n",
    "\n",
    "#Convert some categorical data into numeric\n",
    "\n",
    "df2.loc[:, ['Q1']] = df2.loc[:, ['Q1']].replace({3: 1, 2: 0, 98: 0, 99: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q9']] = df2.loc[:, ['Q9']].replace({2: 0, 3: 0.5, 4: 0.5, 98: 0.5, 99: 0.5})\n",
    "df2.loc[:, ['Q8']] = df2.loc[:, ['Q8']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14A']] = df2.loc[:, ['Q14A']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14B']] = df2.loc[:, ['Q14B']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14C']] = df2.loc[:, ['Q14C']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14D']] = df2.loc[:, ['Q14D']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14E']] = df2.loc[:, ['Q14E']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14F']] = df2.loc[:, ['Q14F']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q14G']] = df2.loc[:, ['Q14G']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q17']] = df2.loc[:, ['Q17']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q22A']] = df2.loc[:, ['Q22A']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q22B']] = df2.loc[:, ['Q22B']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q22C']] = df2.loc[:, ['Q22C']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q22D']] = df2.loc[:, ['Q22D']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q23A']] = df2.loc[:, ['Q23A']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q23B']] = df2.loc[:, ['Q23B']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q23C']] = df2.loc[:, ['Q23C']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q23D']] = df2.loc[:, ['Q23D']].replace({2: 0}).astype('uint8')\n",
    "df2.loc[:, ['Q23E']] = df2.loc[:, ['Q23E']].replace({2: 0}).astype('uint8')\n",
    "\n",
    "df2 = df2.drop(['Gender'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country', 'GlobalRegion', 'CountryIncomeLevel2021', 'Age', 'Education',\n",
       "       'IncomeFeelings', 'INCOME_5', 'EMP_2010', 'Urbanicity', 'HouseholdSize',\n",
       "       'ChildrenInHousehold', 'Q1', 'Q2_1', 'Q3', 'Q4A', 'Q4B', 'Q4C', 'Q4D',\n",
       "       'Q4E', 'Q4F', 'Q4G', 'Q5A', 'Q5B', 'Q5C', 'Q5D', 'Q5E', 'Q5F', 'Q5G',\n",
       "       'Q6', 'Q7A', 'Q7C', 'Q8', 'Q9', 'Q14A', 'Q14B', 'Q14C', 'Q14D', 'Q14E',\n",
       "       'Q14F', 'Q14G', 'Q15', 'Q17', 'Q22A', 'Q22B', 'Q22C', 'Q22D', 'Q22E',\n",
       "       'Q23A', 'Q23B', 'Q23C', 'Q23D', 'Q23E', 'Perception_index',\n",
       "       'Experience_index', 'Impact_index', 'Resilience_index', 'isMale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['Country', 'Region', 'CountryIncomeLevel', 'Age', 'Education', 'IncomePerception',\n",
    "       'IncomeBracket', 'Employment', 'Urbanicity', 'HouseholdSize','Child', 'MoreOrLessSafe','GreatestRisk','ClimateChange',\n",
    "       'WorryFood','WorryWater','WorryCrime','WorryWeather','WorryRoadCrash','WorryMental','WorryWork',\n",
    "       'ExperienceFood','ExperienceWater','ExperienceCrime','ExperienceWeather','ExperienceRoadCrash','ExperienceMental','ExperienceWork',\n",
    "       'UsedInternet','WorryInfoStolen','WorryInfoUsed','AV_Trust','AI_Trust', \n",
    "       'Info_weather', 'Info_agency', 'Info_News', 'Info_religion', 'Info_famous', 'Info_services', 'Info_internet','TrustMost',\n",
    "       'TrustMost','ExperienceDisaster','WithoutElectricity','WithoutWater','WithoutFood','WithoutMedicine','WithoutTelephone',\n",
    "       'DiscriminationSkin', 'DiscriminationReligion', 'DiscriminationEthic', 'DiscriminationGender',\n",
    "       'DiscriminationDisability', 'Perception_ind', 'Experience_ind','Resilience_ind', 'isMale']\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Causal Discovery using a Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian networks (BNs) can be used for causal discovery from observational data. In fact, they are one of the primary tools for this task.\n",
    "\n",
    "A Bayesian network is a directed acyclic graph (DAG) that represents the joint probability distribution over a set of variables. Each node in the graph corresponds to a variable, and the edges between the nodes represent probabilistic dependencies between the variables.\n",
    "\n",
    "Causal discovery is primarily concerned with the first phase of Bayesian network modeling, which is learning the structure or the graphical representation. The learned graph from this phase often represents a causal structure, where the nodes represent variables and the edges represent causal relationships.\n",
    "\n",
    "The structural learning process is able to identify these relationships and the direction of influence based on the patterns and dependencies in the data, which makes it a powerful tool for causal discovery.\n",
    "\n",
    "However, it's important to note that while this structure can suggest causal relationships, it does not definitively prove them. There might be hidden or latent variables not included in the data that can influence the relationships. Therefore, any causal conclusions should be drawn cautiously and, ideally, should be supported by domain knowledge or further experimentation.\n",
    "\n",
    "\n",
    "#### first, let's try HillClimbing Search to develop the DAG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Hill Climb Search to develop the BN structure, then BicScore to estimate the parameters for our BN\n",
    "np.random.seed(123)  # Set the seed to an arbitrary number\n",
    "hc = HillClimbSearch(df2)\n",
    "bic = BicScore(df2)\n",
    "best_model = hc.estimate(scoring_method=bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can visualise the BN\n",
    "# Create a new graph and add edges\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(best_model.edges())\n",
    "\n",
    "# Use pygraphviz to create the layout\n",
    "pos = graphviz_layout(G, prog='dot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, arrowsize=5, node_size=800, node_color = '#87CEEB', alpha=1, font_size=7, ax=ax)\n",
    "\n",
    "plt.savefig('../Figures/causalnetwork.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "\n",
    "# assign colors to nodes based on their names\n",
    "colors = []\n",
    "for node in G.nodes():\n",
    "    if 'Without' in node:\n",
    "        colors.append('#9999FF')\n",
    "    elif '_ind' in node:\n",
    "        colors.append('#CC99FF')\n",
    "    elif 'Info_' in node:\n",
    "        colors.append('#FF7DFF')\n",
    "    elif 'Discrimination' in node:\n",
    "        colors.append('#FF9999')\n",
    "    elif 'n_' in node:\n",
    "        colors.append('#CCFFCC')\n",
    "    elif 'o_' in node:\n",
    "        colors.append('#FFFF2B')\n",
    "    else:    \n",
    "        colors.append('#E0E0E0')\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, arrowsize=3, node_size=300, node_color = colors, alpha=0.8, font_size=10, ax=ax)\n",
    "\n",
    "plt.savefig('../Figures/causalnetwork.pdf',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/DAG.gpickle\", 'wb') as f:\n",
    "    pickle.dump(G, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load later by using:\n",
    "#with open(\"../Data/DAG.gpickle\", 'rb') as f:\n",
    "#    G = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to learn the DAG structure using the PC (Constraint-based Search) instead\n",
    "\n",
    "### Note: Only run on a HPC as this method takes forever! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pgmpy.estimators import PC\n",
    "\n",
    "# Use Hill Climb Search to develop the BN structure, then BicScore to estimate the parameters for our BN\n",
    "#hc = PC(df2)\n",
    "#bic = BicScore(df2)\n",
    "#best_model = hc.estimate(scoring_method=bic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Estimating Causal Effects with DoWhy \n",
    "\n",
    "Code adopted from: https://microsoft.github.io/dowhy/example_notebooks/dowhy-conditional-treatment-effects.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: causal model for Experience index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the finished DAG if you have run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load later by using:\n",
    "with open(\"../Data/DAG.gpickle\", 'rb') as f:\n",
    "    G = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = \"\"\"\n",
    "digraph {\n",
    "    Safer -> Dis_ethic;\n",
    "    Safer -> Experience_ind;\n",
    "    Dis_ethic->Experience_ind;\n",
    "    Dis_gender->Dis_ethic;\n",
    "    Dis_religion -> Dis_gender;\n",
    "    Dis_religion -> Dis_skin;\n",
    "    Dis_religion -> Dis_ethic;\n",
    "    Dis_skin -> Dis_ethic;\n",
    "    Dis_gender->Dis_disability;\n",
    "    Dis_skin->Dis_disability;\n",
    "    Dis_religion->Dis_disability;\n",
    "}\n",
    "\"\"\"\n",
    "model = CausalModel(\n",
    "    data=df2,\n",
    "    treatment='Dis_ethic',\n",
    "    outcome='Experience_ind',\n",
    "    graph=graph\n",
    ")\n",
    "\n",
    "# View model\n",
    "model.view_model()\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Identifying the estimand\n",
    "identified_estimand = model.identify_effect()\n",
    "estimate = model.estimate_effect(identified_estimand, \n",
    "                                 method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                                 target_units=\"ate\",\n",
    "                                 method_params={\"init_params\":{'models': RandomForestRegressor()},\n",
    "                                                \"fit_params\":{}})\n",
    "# Printing the causal effect\n",
    "print(estimate.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print histogram of causal effects for each sample\n",
    "plt.hist(estimate.cate_estimates,bins=10)\n",
    "\n",
    "CATE_experience = estimate.cate_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Causal estimation for Perception index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Employment_binary'] = df2['Employment'].apply(lambda x: 1 if x < 3 else 0).astype('uint8')\n",
    "\n",
    "graph = \"\"\"\n",
    "digraph {\n",
    "    Age -> Employment_binary;\n",
    "    Employment_binary -> Perception_ind;\n",
    "    isMale->Employment_binary; \n",
    "    Region-> Perception_ind;\n",
    "    Region -> isMale;\n",
    "    Country->Region;\n",
    "}\n",
    "\"\"\"\n",
    "model = CausalModel(\n",
    "    data=df2,\n",
    "    treatment='Employment_binary',\n",
    "    outcome='Perception_ind',\n",
    "    graph=graph\n",
    ")\n",
    "\n",
    "# View model\n",
    "model.view_model()\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identifying the estimand\n",
    "identified_estimand = model.identify_effect()\n",
    "estimate = model.estimate_effect(identified_estimand, \n",
    "                                 method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                                 target_units=\"ate\",\n",
    "                                 method_params={\"init_params\":{'models': RandomForestRegressor()},\n",
    "                                                \"fit_params\":{}})\n",
    "# Printing the causal effect\n",
    "print(estimate.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print histogram of causal effects for each sample\n",
    "plt.hist(estimate.cate_estimates,bins=10)\n",
    "\n",
    "CATE_perception = estimate.cate_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Causal estimation for Resilience index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['IncomePerc_binary'] = df2['IncomePerc'].apply(lambda x: 1 if x < 3 else 0).astype('uint8')\n",
    "#df2['IncomePerc_binary'] = df2['IncomePerc']\n",
    "graph = \"\"\"\n",
    "digraph {\n",
    "    Child->IncomeBracket\n",
    "    IncomePerc -> Resilience_ind;\n",
    "    IncomeBracket -> IncomePerc;\n",
    "    Country->IncomePerc;\n",
    "    Country-> Child;\n",
    "    }\n",
    "\"\"\"\n",
    "#Country->IncomePerc_binary; ;\n",
    "#Country-> Child;\n",
    "\n",
    "model = CausalModel(\n",
    "    data=df2,\n",
    "    treatment='IncomePerc_binary',\n",
    "    outcome='Resilience_ind',\n",
    "    graph=graph\n",
    ")\n",
    "\n",
    "# View model\n",
    "model.view_model()\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.metalearners import TLearner\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `df` is your DataFrame and `cat_var` is your categorical variable\n",
    "df_encoded = pd.get_dummies(df2['Country'])\n",
    "\n",
    "# Choose a set of control variables\n",
    "X = pd.concat([df2[['IncomeBracket','Child']], df_encoded], axis=1)\n",
    "\n",
    "#df2[['IncomeBracket','Child']]\n",
    "\n",
    "# Choose the treatment variable\n",
    "T = df2['IncomePerc_binary']\n",
    "\n",
    "# Choose the outcome variable\n",
    "Y = df2['Resilience_ind']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test, T_train, T_test = train_test_split(X, Y, T, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the learner models\n",
    "learner = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the TLearner class with the learner models\n",
    "T_learner = TLearner(models=learner)\n",
    "\n",
    "# Reshape treatment to be 2D\n",
    "T_train_2D = np.array(T_train).reshape(-1, 1)\n",
    "\n",
    "# Fit the model\n",
    "T_learner.fit(Y=Y_train, T=T_train_2D, X=X_train)\n",
    "\n",
    "# Estimate the CATE with the test set\n",
    "cate_test = T_learner.effect(X=X_test)\n",
    "\n",
    "# Print the CATE\n",
    "print(cate_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the CATE with the test set\n",
    "CATE_resilience = T_learner.effect(X=X)\n",
    "\n",
    "# print histogram of causal effects for each sample\n",
    "plt.hist(CATE_resilience,bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ATE\n",
    "ate = np.mean(CATE_resilience)\n",
    "\n",
    "# Print the ATE\n",
    "print(\"ATE:\", ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Perform refutation sensitivity analysis with resilience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.interpreters import SingleTreeCateInterpreter\n",
    "\n",
    "# Instantiate the SingleTreeCateInterpreter class\n",
    "interpreter = SingleTreeCateInterpreter(include_model_uncertainty=True)\n",
    "\n",
    "# Perform refutation sensitivity analysis\n",
    "refutation_results = interpreter.interpret(T_learner, X_train, alpha=0.05, n_bootstraps=100)\n",
    "\n",
    "# Print the results\n",
    "print(\"Refutation Results:\")\n",
    "print(refutation_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sensitivity Analysis*: The estimate object also has a sensitivity method that can be used to conduct a sensitivity analysis. This can tell you how robust your results are to potential violations of the assumptions behind causal inference (e.g., the unconfoundedness assumption).\n",
    "\n",
    "The sensitivity analysis can give you more confidence in your results or suggest that they should be interpreted with caution.\n",
    "\n",
    "Remember that all of these results are based on the assumptions you made when setting up your causal model, including the choice of treatment and outcome variables, the selection of covariates, and the assumption of unconfoundedness. Always critically assess these assumptions when interpreting your results.\n",
    "\n",
    "\n",
    "- Refute: Add a random common cause\n",
    "- Estimated effect:-0.049451366969192764\n",
    "- New effect:-0.05351417843313906\n",
    "- p value:0.6399999999999999\n",
    "\n",
    "The sensitivity analysis you've run, \"Add a random common cause\", is a method of testing the robustness of your causal effect estimate. \n",
    "\n",
    "The idea behind this test is to add a random variable (which should theoretically be unrelated to any other variables in the data) to your model as a common cause (a variable that influences both the treatment and outcome). If the causal effect estimate changes substantially with the addition of this random variable, it suggests that the original estimate may not be robust and could be susceptible to unobserved confounding. In other words, there could be some omitted variable that, if included in the model, would change the estimated causal effect.\n",
    "\n",
    "In your case, the estimated effect was originally -0.04945, and with the addition of a random common cause, the effect is -0.05351. The effect has changed slightly but not dramatically. \n",
    "\n",
    "The p-value represents the statistical significance of the difference between the original estimate and the new estimate. A p-value greater than 0.05 (common threshold) typically indicates that the difference is not statistically significant. In your case, with a p-value of approximately 0.64, we would not consider the change in the estimated effect to be statistically significant.\n",
    "\n",
    "Given these results, it seems like your original causal estimate is relatively robust to the addition of a random common cause. This suggests that your findings might also be robust to some potential omitted variables. However, remember that no sensitivity analysis can fully guard against the risk of unobserved confounding, and they are a way to gauge, not confirm, the robustness of your results. \n",
    "\n",
    "Always interpret the results in the context of your knowledge about the subject matter, as well as the limitations of your data and the assumptions of your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conditional Average Treatment Effect (CATE) refers to the average treatment effect conditional on covariates or features. In other words, instead of a single average effect across the entire population (as in ATE), CATE allows us to understand how the treatment effect varies across different subgroups or individuals, based on their characteristics.\n",
    "\n",
    "For example, consider a scenario where we're evaluating an educational intervention. The ATE would tell us the average impact of the intervention on students' grades, while the CATE could tell us how the impact of the intervention differs between high school and elementary school students.\n",
    "\n",
    "In the context of your data, once you have an estimate of the CATE, you can examine the treatment effect for different subgroups of individuals defined by their covariates.\n",
    "\n",
    "You could, for instance, plot the estimated CATE against one or more covariates to visually inspect how the treatment effect varies with these variables. Another approach might be to calculate summary statistics (mean, median, etc.) of the estimated CATE for different subgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the causal inference results\n",
    "\n",
    "Note that the causal network DAG has already been saved to `DAG.gpickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../Data/CATE_perception.csv\", CATE_perception, delimiter=\",\")\n",
    "np.savetxt(\"../Data/CATE_experience.csv\", CATE_experience, delimiter=\",\")\n",
    "np.savetxt(\"../Data/CATE_resilience.csv\", CATE_resilience, delimiter=\",\")\n",
    "\n",
    "#also save the dataframe to pickle\n",
    "df2.to_pickle('../Data/df_processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('causal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cb7cc054190f6a9f61a488deded6b046aa8b6fd262ced6af83b58d1c06e36a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
